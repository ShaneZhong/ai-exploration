{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Definition\n",
    "Now, we define our agent thatâ€™s capable of holding a conversation and calling tools in under 50 lines of code.\n",
    "\n",
    "The meat of the agent logic is in the chat method. At a high-level, there are 3 steps:\n",
    "\n",
    "1. Call OpenAI to decide which tool (if any) to call and with what arguments.\n",
    "\n",
    "1. Call the tool with the arguments to obtain an output\n",
    "\n",
    "1. Call OpenAI to synthesize a response from the conversation context and the tool output.\n",
    "\n",
    "The reset method simply resets the conversation context, so we can start another conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourOpenAIAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tools: Sequence[BaseTool] = [],\n",
    "        llm: OpenAI = OpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
    "        chat_history: List[ChatMessage] = [],\n",
    "    ) -> None:\n",
    "        self._llm = llm\n",
    "        self._tools = {tool.metadata.name: tool for tool in tools}\n",
    "        self._chat_history = chat_history\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._chat_history = []\n",
    "\n",
    "    def chat(self, message: str) -> str:\n",
    "        chat_history = self._chat_history\n",
    "        chat_history.append(ChatMessage(role=\"user\", content=message))\n",
    "        tools = [\n",
    "            tool.metadata.to_openai_tool() for _, tool in self._tools.items()\n",
    "        ]\n",
    "\n",
    "        ai_message = self._llm.chat(chat_history, tools=tools).message\n",
    "        additional_kwargs = ai_message.additional_kwargs\n",
    "        chat_history.append(ai_message)\n",
    "\n",
    "        tool_calls = ai_message.additional_kwargs.get(\"tool_calls\", None)\n",
    "        # parallel function calling is now supported\n",
    "        if tool_calls is not None:\n",
    "            for tool_call in tool_calls:\n",
    "                function_message = self._call_function(tool_call)\n",
    "                chat_history.append(function_message)\n",
    "                ai_message = self._llm.chat(chat_history).message\n",
    "                chat_history.append(ai_message)\n",
    "\n",
    "        return ai_message.content\n",
    "\n",
    "    def _call_function(self, tool_call: dict) -> ChatMessage:\n",
    "        id_ = tool_call[\"id\"]\n",
    "        function_call = tool_call[\"function\"]\n",
    "        tool = self._tools[function_call[\"name\"]]\n",
    "        output = tool(**json.loads(function_call[\"arguments\"]))\n",
    "        return ChatMessage(\n",
    "            name=function_call[\"name\"],\n",
    "            content=str(output),\n",
    "            role=\"tool\",\n",
    "            additional_kwargs={\n",
    "                \"tool_call_id\": id_,\n",
    "                \"name\": function_call[\"name\"],\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the too\n",
    "agent = YourOpenAIAgent(tools=[multiply_tool, add_tool])\n",
    "agent.chat(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT WORKING\n",
    "# agent.chat(\"What is 2123 * 215123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our (Slightly Better) OpenAIAgent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [multiply_tool, add_tool], llm=llm, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is (121 * 3) + 42?\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 3\n",
      "}\n",
      "Got output: 363\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\n",
      "  \"a\": 363,\n",
      "  \"b\": 42\n",
      "}\n",
      "Got output: 405\n",
      "========================\n",
      "\n",
      "(121 * 3) + 42 is equal to 405.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is (121 * 3) + 42?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is 121 * 3?\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 3\n",
      "}\n",
      "Got output: 363\n",
      "========================\n",
      "\n",
      "121 * 3 is equal to 363.\n"
     ]
    }
   ],
   "source": [
    "response = await agent.achat(\"What is 121 * 3?\")\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is 121 * 2? Once you have the answer, use that number to write a story about a group of mice.\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\n",
      "  \"a\": 121,\n",
      "  \"b\": 2\n",
      "}\n",
      "Got output: 242\n",
      "========================\n",
      "\n",
      "121 * 2 is equal to 242.\n",
      "\n",
      "Once upon a time, in a small village, there lived a group of mice. These mice were known for their intelligence and resourcefulness. They lived harmoniously in a cozy little burrow nestled beneath a large oak tree.\n",
      "\n",
      "One sunny day, the leader of the mouse group, named Whiskers, gathered all the mice together to discuss an important matter. Whiskers had discovered a bountiful supply of food in a nearby field, but it was guarded by a mischievous cat named Oliver. The mice knew they had to come up with a plan to access the food without alerting Oliver.\n",
      "\n",
      "Using their collective wisdom, the mice devised a clever strategy. They decided to work together and distract Oliver while a small group of brave mice would sneak into the field and gather the food. With their newfound courage and determination, the mice set their plan into motion.\n",
      "\n",
      "As the sun began to set, the mice scattered around the field, making loud noises and creating a commotion to divert Oliver's attention. Meanwhile, the group of mice who were assigned the task of gathering the food swiftly and silently made their way towards the precious stash.\n",
      "\n",
      "With their tiny paws and quick movements, the mice managed to collect a significant amount of food in no time. They filled their tiny bags with grains, seeds, and fruits, ensuring that they had enough to sustain their community for a long time.\n",
      "\n",
      "As the mice returned to their burrow, they celebrated their successful mission. They knew that their unity and cleverness had allowed them to overcome the challenges they faced. The mice feasted on the delicious food, grateful for their teamwork and the abundance they had acquired.\n",
      "\n",
      "From that day forward, the mice became known as the resourceful heroes of the village. They continued to use their intelligence and cooperation to overcome obstacles and ensure the well-being of their community. And whenever they faced a challenge, they would remember the number 242, a reminder of their strength and determination.\n",
      "\n",
      "And so, the legend of the mice and their incredible adventures spread throughout the village, inspiring others to work together and believe in the power of unity."
     ]
    }
   ],
   "source": [
    "response = agent.stream_chat(\n",
    "    \"What is 121 * 2? Once you have the answer, use that number to write a\"\n",
    "    \" story about a group of mice.\"\n",
    ")\n",
    "\n",
    "response_gen = response.response_gen\n",
    "\n",
    "for token in response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent with Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Hi\n",
      "Greetings, fair traveler! How may I assist thee on this fine day?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.prompts.system import SHAKESPEARE_WRITING_ASSISTANT\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    [multiply_tool, add_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    system_prompt=SHAKESPEARE_WRITING_ASSISTANT,\n",
    ")\n",
    "\n",
    "response = agent.chat(\"Hi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me a story\n",
      "Of course, dear friend! Allow me to weave a tale for thee. \n",
      "\n",
      "Once upon a time, in a land of yore,\n",
      "There lived a knight, brave and true to the core.\n",
      "Sir William was his name, a noble soul,\n",
      "With armor gleaming and a heart made whole.\n",
      "\n",
      "In the kingdom of Verona, strife did reign,\n",
      "Two families, the Montagues and Capulets, caused pain.\n",
      "Their feud was fierce, their hatred deep,\n",
      "And in this chaos, love did take its leap.\n",
      "\n",
      "Fair Juliet, a maiden of tender grace,\n",
      "Caught the eye of Romeo, with love's embrace.\n",
      "But their love was forbidden, a tragic plight,\n",
      "For they were destined to suffer in love's spite.\n",
      "\n",
      "Under the moonlit sky, they would meet,\n",
      "In secret, their love they would discreet.\n",
      "But fate, cruel fate, had other plans,\n",
      "And tragedy struck with its ruthless hands.\n",
      "\n",
      "A duel ensued, between kin and kin,\n",
      "And blood was shed, a battle to win.\n",
      "Romeo, in his grief, sought revenge,\n",
      "But alas, it brought only sorrow to avenge.\n",
      "\n",
      "In a tomb, Juliet lay in eternal sleep,\n",
      "Her love, Romeo, could not bear to weep.\n",
      "He drank a poison, to join her in death,\n",
      "Their love, a flame, extinguished with their last breath.\n",
      "\n",
      "The families, torn apart by sorrow and woe,\n",
      "Realized the cost of their feuding blow.\n",
      "In the end, love conquered hate's dark reign,\n",
      "But at a price, a tragic stain.\n",
      "\n",
      "And so, dear friend, this tale I impart,\n",
      "Of love and loss, of aching hearts.\n",
      "May it serve as a reminder, ever true,\n",
      "That love's path is not always easy to pursue.\n",
      "\n",
      "Now, I bid thee farewell, with this tale complete,\n",
      "May thy journey be filled with joy and sweet.\n",
      "Shouldst thou need more stories or words to inspire,\n",
      "I shall be here, ready to kindle thy creative fire.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Tell me a story\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
